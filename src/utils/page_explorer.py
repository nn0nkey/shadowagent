"""
é¡µé¢æ¢ç´¢å·¥å…·
è‡ªåŠ¨æ”¶é›†ç›®æ ‡ç½‘ç«™çš„é¡µé¢ä¿¡æ¯ã€APIç«¯ç‚¹ã€JSæ–‡ä»¶ç­‰
"""
import re
import subprocess
import requests
from urllib.parse import urljoin, urlparse
from typing import Dict, List, Optional
from src.utils.logger import default_logger
import json


def _can_access_url(url: str) -> bool:
    """
    æ£€æµ‹å½“å‰ç¯å¢ƒèƒ½å¦ç›´æ¥è®¿é—® URL
    
    Returns:
        True: å¯ä»¥ç›´æ¥ç”¨ requests è®¿é—®
        False: éœ€è¦é€šè¿‡ Docker å®¹å™¨è®¿é—®
    """
    try:
        # å¿«é€Ÿæµ‹è¯•è¿æ¥ï¼ˆ2ç§’è¶…æ—¶ï¼Œé¿å…è¯¯åˆ¤ï¼‰
        response = requests.get(url, timeout=2)
        return response.status_code < 500  # åªè¦ä¸æ˜¯æœåŠ¡å™¨é”™è¯¯å°±ç®—æˆåŠŸ
    except:
        return False


def _get_page_content(url: str) -> str:
    """
    è·å–é¡µé¢å†…å®¹ï¼ˆè‡ªåŠ¨é€‰æ‹©è®¿é—®æ–¹å¼ï¼‰
    
    Returns:
        é¡µé¢ HTML å†…å®¹
    """
    # æ£€æµ‹æ˜¯å¦èƒ½ç›´æ¥è®¿é—®
    can_access = _can_access_url(url)
    default_logger.info(f"ğŸŒ [é¡µé¢è®¿é—®] URL: {url}, æœ¬åœ°å¯è®¿é—®: {can_access}")
    
    if can_access:
        # æœ¬åœ°ç¯å¢ƒå¯ä»¥ç›´æ¥è®¿é—®
        default_logger.info(f"ğŸŒ [é¡µé¢è®¿é—®] ä½¿ç”¨æœ¬åœ° requests")
        response = requests.get(url, timeout=10, allow_redirects=True)
        html = response.text
        default_logger.info(f"ğŸŒ [é¡µé¢è®¿é—®] è·å–åˆ° {len(html)} å­—ç¬¦")
        return html
    else:
        # éœ€è¦é€šè¿‡ Docker å®¹å™¨è®¿é—®ï¼ˆå¦‚ host.docker.internalï¼‰
        # ä½¿ç”¨åŒæ­¥ subprocess é¿å…äº‹ä»¶å¾ªç¯å†²çª
        default_logger.info(f"ğŸŒ [é¡µé¢è®¿é—®] ä½¿ç”¨ Docker å®¹å™¨ curl")
        import subprocess
        try:
            result = subprocess.run(
                ["docker", "exec", "shadowagent-kali", "curl", "-s", "-L", url],
                capture_output=True,
                text=True,
                timeout=10
            )
            html = result.stdout
            default_logger.info(f"ğŸŒ [é¡µé¢è®¿é—®] è·å–åˆ° {len(html)} å­—ç¬¦")
            return html
        except Exception as e:
            default_logger.error(f"ğŸŒ [é¡µé¢è®¿é—®] Docker curl å¤±è´¥: {e}")
            return ""


def explore_target_initial(url: str, timeout: int = 60) -> Dict:
    """
    åˆå§‹æ¢ç´¢ï¼šè‡ªåŠ¨æ”¶é›†é¡µé¢ä¿¡æ¯
    
    Args:
        url: ç›®æ ‡URL
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        æ¢ç´¢ç»“æœå­—å…¸
    """
    default_logger.info(f"ğŸ” [é¡µé¢æ¢ç´¢] å¼€å§‹æ¢ç´¢ç›®æ ‡: {url}")
    
    result = {
        'base_info': {},
        'paths': [],
        'js_files': [],
        'api_endpoints': [],
        'forms': [],
        'links': [],
        'page_content': ''  # æ·»åŠ é¡µé¢å†…å®¹å­—æ®µï¼ˆä¾› HAE è§£æï¼‰
    }
    
    try:
        # 1. è·å–åŸºç¡€ä¿¡æ¯
        default_logger.info("ğŸ“‹ [é¡µé¢æ¢ç´¢] è·å–åŸºç¡€ä¿¡æ¯...")
        result['base_info'] = _get_base_info(url)
        
        # 2. æ£€æŸ¥ API æ–‡æ¡£ï¼ˆopenapi.json, /docs, /swaggerï¼‰
        default_logger.info("ğŸ“š [é¡µé¢æ¢ç´¢] æ£€æŸ¥ API æ–‡æ¡£...")
        api_docs = _check_api_docs(url)
        if api_docs:
            result['api_endpoints'].extend(api_docs)
            default_logger.info(f"âœ… [é¡µé¢æ¢ç´¢] ä» API æ–‡æ¡£å‘ç° {len(api_docs)} ä¸ªç«¯ç‚¹")
        
        # 3. å¿«é€Ÿè·¯å¾„æ‰«æï¼ˆä½¿ç”¨ dirbï¼Œåªæ‰«æ commonï¼‰
        default_logger.info("ğŸ” [é¡µé¢æ¢ç´¢] å¿«é€Ÿè·¯å¾„æ‰«æ...")
        result['paths'] = _quick_path_scan(url, timeout=timeout)
        default_logger.info(f"âœ… [é¡µé¢æ¢ç´¢] å‘ç° {len(result['paths'])} ä¸ªæœ‰æ•ˆè·¯å¾„")
        
        # 4. æå–é¡µé¢å†…å®¹ï¼ˆJSã€é“¾æ¥ã€è¡¨å•ï¼‰
        default_logger.info("ğŸ“„ [é¡µé¢æ¢ç´¢] æå–é¡µé¢å†…å®¹...")
        page_content_data = _extract_page_content(url)
        result['js_files'] = page_content_data.get('js_files', [])
        result['links'] = page_content_data.get('links', [])
        result['forms'] = page_content_data.get('forms', [])
        result['page_content'] = page_content_data.get('html', '')  # ä¿å­˜åŸå§‹ HTML
        default_logger.info(f"âœ… [é¡µé¢æ¢ç´¢] æå–åˆ° {len(result['js_files'])} ä¸ª JS æ–‡ä»¶, {len(result['links'])} ä¸ªé“¾æ¥, {len(result['forms'])} ä¸ªè¡¨å•")
        
        # 5. åˆ†æ JS æ–‡ä»¶ï¼ˆä½¿ç”¨ linkfinderï¼‰
        if result['js_files']:
            default_logger.info("ğŸ”¬ [é¡µé¢æ¢ç´¢] åˆ†æ JS æ–‡ä»¶...")
            for js_url in result['js_files'][:5]:  # æœ€å¤šåˆ†æ5ä¸ª
                endpoints = _analyze_js_file(js_url)
                if endpoints:
                    result['api_endpoints'].extend(endpoints)
            default_logger.info(f"âœ… [é¡µé¢æ¢ç´¢] ä» JS æ–‡ä»¶å‘ç° {len(result['api_endpoints'])} ä¸ª API ç«¯ç‚¹")
        
        default_logger.info(f"ğŸ‰ [é¡µé¢æ¢ç´¢] æ¢ç´¢å®Œæˆï¼æ€»è®¡: {len(result['paths'])} è·¯å¾„, {len(result['api_endpoints'])} API, {len(result['js_files'])} JS")
        
    except Exception as e:
        default_logger.error(f"âŒ [é¡µé¢æ¢ç´¢] æ¢ç´¢å¤±è´¥: {e}")
    
    return result


def _get_base_info(url: str) -> Dict:
    """
    è·å–ç›®æ ‡çš„åŸºç¡€ä¿¡æ¯
    """
    info = {
        'url': url,
        'status_code': None,
        'title': None,
        'server': None,
        'tech_stack': []
    }
    
    try:
        # ä½¿ç”¨ç»Ÿä¸€çš„é¡µé¢è·å–æ–¹æ³•
        html = _get_page_content(url)
        
        # æå–æ ‡é¢˜
        title_match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)
        if title_match:
            info['title'] = title_match.group(1).strip()
        
        # å°è¯•ä»å“åº”å¤´è·å–ä¿¡æ¯ï¼ˆå¦‚æœæ˜¯æœ¬åœ°è®¿é—®ï¼‰
        if _can_access_url(url):
            response = requests.get(url, timeout=10, allow_redirects=True)
            info['status_code'] = response.status_code
            info['server'] = response.headers.get('Server', 'Unknown')
            
            # è¯†åˆ«æŠ€æœ¯æ ˆ
            tech_stack = []
            if 'X-Powered-By' in response.headers:
                tech_stack.append(response.headers['X-Powered-By'])
            
            # æ£€æµ‹æ¡†æ¶
            if 'uvicorn' in response.headers.get('Server', '').lower():
                tech_stack.append('FastAPI/Uvicorn')
            elif 'werkzeug' in response.headers.get('Server', '').lower():
                tech_stack.append('Flask/Werkzeug')
            elif 'express' in response.headers.get('X-Powered-By', '').lower():
                tech_stack.append('Express.js')
            
            info['tech_stack'] = tech_stack
        
    except Exception as e:
        default_logger.warning(f"è·å–åŸºç¡€ä¿¡æ¯å¤±è´¥: {e}")
    
    return info


def _check_api_docs(url: str) -> List[str]:
    """
    æ£€æŸ¥å¸¸è§çš„ API æ–‡æ¡£ç«¯ç‚¹
    """
    api_endpoints = []
    parsed_url = urlparse(url)
    base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
    
    # å¸¸è§çš„ API æ–‡æ¡£è·¯å¾„
    doc_paths = [
        '/openapi.json',
        '/api/openapi.json',
        '/docs',
        '/api/docs',
        '/swagger.json',
        '/api/swagger.json',
        '/swagger',
        '/api-docs',
        '/redoc'
    ]
    
    for path in doc_paths:
        try:
            doc_url = urljoin(base_url, path)
            response = requests.get(doc_url, timeout=5)
            
            if response.status_code == 200:
                # å°è¯•è§£æ OpenAPI/Swagger JSON
                if path.endswith('.json'):
                    try:
                        api_spec = response.json()
                        if 'paths' in api_spec:
                            endpoints = list(api_spec['paths'].keys())
                            default_logger.info(f"âœ… ä» {path} å‘ç° {len(endpoints)} ä¸ªç«¯ç‚¹")
                            api_endpoints.extend(endpoints)
                    except json.JSONDecodeError:
                        pass
                else:
                    default_logger.info(f"âœ… å‘ç° API æ–‡æ¡£: {doc_url}")
        except:
            pass
    
    return list(set(api_endpoints))  # å»é‡


def _quick_path_scan(url: str, timeout: int = 60) -> List[str]:
    """
    å¿«é€Ÿè·¯å¾„æ‰«æ
    
    æ³¨æ„ï¼šåˆå§‹æ¢ç´¢åœ¨æœ¬åœ°ç¯å¢ƒæ‰§è¡Œï¼Œä¸åœ¨ Docker å®¹å™¨ä¸­
    å› æ­¤ä¸ä½¿ç”¨ dirb/gobuster ç­‰å·¥å…·ï¼Œè€Œæ˜¯ä½¿ç”¨ç®€å•çš„è·¯å¾„æ¢æµ‹
    è¯¦ç»†çš„ç›®å½•æ‰«æç”± Agent åœ¨ Docker å®¹å™¨ä¸­æ‰§è¡Œ
    """
    paths = []
    
    default_logger.info(f"å¿«é€Ÿè·¯å¾„æ¢æµ‹: {url}")
    
    # å¸¸è§çš„é‡è¦è·¯å¾„
    common_paths = [
        # è®¤è¯ç›¸å…³
        '/login', '/signin', '/auth', '/token', '/logout',
        # ç®¡ç†ç›¸å…³
        '/admin', '/dashboard', '/console', '/manage',
        # API ç›¸å…³
        '/api', '/api/v1', '/api/v2', '/graphql',
        # æ–‡æ¡£ç›¸å…³
        '/docs', '/swagger', '/redoc', '/api-docs',
        # é…ç½®å’ŒçŠ¶æ€
        '/config', '/status', '/health', '/metrics',
        # ç”¨æˆ·ç›¸å…³
        '/users', '/user', '/profile', '/account',
        # å…¶ä»–
        '/register', '/signup', '/reset', '/forgot'
    ]
    
    valid_statuses = [200, 201, 202, 204, 301, 302, 303, 307, 308, 401, 403]
    
    for path in common_paths:
        try:
            test_url = urljoin(url, path)
            response = requests.get(test_url, timeout=3, allow_redirects=False)
            if response.status_code in valid_statuses:
                paths.append(f"{path} [Status: {response.status_code}]")
                default_logger.debug(f"âœ“ {path} [{response.status_code}]")
        except requests.exceptions.Timeout:
            default_logger.debug(f"âœ— {path} [Timeout]")
        except requests.exceptions.ConnectionError:
            default_logger.debug(f"âœ— {path} [Connection Error]")
        except Exception as e:
            default_logger.debug(f"âœ— {path} [{type(e).__name__}]")
    
    default_logger.info(f"å¿«é€Ÿæ¢æµ‹å®Œæˆï¼Œå‘ç° {len(paths)} ä¸ªè·¯å¾„")
    return paths


def _extract_page_content(url: str) -> Dict:
    """
    ä»é¡µé¢æå– JS æ–‡ä»¶ã€é“¾æ¥ã€è¡¨å•
    """
    content = {
        'js_files': [],
        'links': [],
        'forms': [],
        'html': ''  # ä¿å­˜åŸå§‹ HTMLï¼ˆä¾› HAE è§£æï¼‰
    }
    
    try:
        # ä½¿ç”¨ç»Ÿä¸€çš„é¡µé¢è·å–æ–¹æ³•
        html = _get_page_content(url)
        content['html'] = html  # ä¿å­˜åŸå§‹ HTML
        parsed_url = urlparse(url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
        
        # 1. æå– JS æ–‡ä»¶
        js_patterns = [
            r'<script[^>]+src=["\']([^"\']+)["\']',  # <script src="...">
            r'src=["\']([^"\']*\.js[^"\']*)["\']',   # src="...js..."
        ]
        
        js_urls = set()
        for pattern in js_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            for match in matches:
                if match and not match.startswith('data:'):
                    # å¤„ç†ç›¸å¯¹è·¯å¾„
                    if match.startswith('http'):
                        js_urls.add(match)
                    elif match.startswith('/'):
                        js_urls.add(base_url + match)
                    else:
                        js_urls.add(urljoin(url, match))
        
        content['js_files'] = list(js_urls)
        
        # 2. æå–é“¾æ¥
        link_pattern = r'<a[^>]+href=["\']([^"\']+)["\']'
        links = re.findall(link_pattern, html, re.IGNORECASE)
        
        # è¿‡æ»¤å’Œè§„èŒƒåŒ–é“¾æ¥
        valid_links = []
        for link in links:
            if link and not link.startswith(('#', 'javascript:', 'mailto:')):
                if link.startswith('http'):
                    valid_links.append(link)
                elif link.startswith('/'):
                    valid_links.append(base_url + link)
                else:
                    valid_links.append(urljoin(url, link))
        
        content['links'] = list(set(valid_links))
        
        # 3. æå–è¡¨å•
        # ä¿®æ”¹æ­£åˆ™ï¼šåŒæ—¶æ•è· <form> æ ‡ç­¾å’Œè¡¨å•å†…å®¹
        form_pattern = r'<form([^>]*)>(.*?)</form>'
        forms = re.findall(form_pattern, html, re.IGNORECASE | re.DOTALL)
        
        for form_tag, form_content in forms:
            form_info = {}
            
            # ä» <form> æ ‡ç­¾ä¸­æå– action
            action_match = re.search(r'action=["\']([^"\']+)["\']', form_tag, re.IGNORECASE)
            if action_match:
                action = action_match.group(1)
                if action.startswith('http'):
                    form_info['action'] = action
                elif action.startswith('/'):
                    form_info['action'] = base_url + action
                else:
                    form_info['action'] = urljoin(url, action)
            else:
                form_info['action'] = url  # é»˜è®¤æäº¤åˆ°å½“å‰é¡µé¢
            
            # ä» <form> æ ‡ç­¾ä¸­æå– method
            method_match = re.search(r'method=["\']([^"\']+)["\']', form_tag, re.IGNORECASE)
            form_info['method'] = method_match.group(1).upper() if method_match else 'GET'
            
            # ä»è¡¨å•å†…å®¹ä¸­æå–è¾“å…¥å­—æ®µ
            input_pattern = r'<input[^>]+name=["\']([^"\']+)["\']'
            inputs = re.findall(input_pattern, form_content, re.IGNORECASE)
            form_info['inputs'] = inputs
            
            if form_info.get('inputs'):
                content['forms'].append(form_info)
        
    except Exception as e:
        default_logger.warning(f"æå–é¡µé¢å†…å®¹å¤±è´¥: {e}")
    
    return content


def _analyze_js_file(js_url: str) -> List[str]:
    """
    åˆ†æ JS æ–‡ä»¶ï¼Œæå– API ç«¯ç‚¹
    ä¼˜å…ˆä½¿ç”¨ linkfinderï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
    """
    endpoints = []
    
    # å°è¯•ä½¿ç”¨ linkfinder
    try:
        check_linkfinder = subprocess.run(['which', 'linkfinder'], capture_output=True, timeout=5)
        if check_linkfinder.returncode == 0:
            cmd = [
                'linkfinder',
                '-i', js_url,
                '-o', 'cli'
            ]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            # è§£æ linkfinder è¾“å‡º
            for line in result.stdout.split('\n'):
                line = line.strip()
                if line and line.startswith('/'):
                    # è¿‡æ»¤æ‰é™æ€èµ„æº
                    if not any(line.endswith(ext) for ext in ['.css', '.png', '.jpg', '.ico', '.svg', '.woff']):
                        endpoints.append(line)
            
            if endpoints:
                return list(set(endpoints))
    except:
        pass
    
    # linkfinder ä¸å¯ç”¨ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†æ
    try:
        default_logger.debug(f"ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†æ JS: {js_url}")
        response = requests.get(js_url, timeout=10)
        js_content = response.text
        
        # å¸¸è§çš„ API è·¯å¾„æ¨¡å¼
        patterns = [
            r'["\']/(api/[a-zA-Z0-9/_-]+)["\']',  # /api/...
            r'["\']/(v\d+/[a-zA-Z0-9/_-]+)["\']',  # /v1/...
            r'["\'](/[a-zA-Z0-9/_-]+)["\']',       # é€šç”¨è·¯å¾„
            r'endpoint\s*[:=]\s*["\']([^"\']+)["\']',  # endpoint: "..."
            r'url\s*[:=]\s*["\']([^"\']+)["\']',       # url: "..."
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, js_content)
            for match in matches:
                if match.startswith('/') and len(match) > 1:
                    # è¿‡æ»¤é™æ€èµ„æº
                    if not any(match.endswith(ext) for ext in ['.css', '.png', '.jpg', '.ico', '.svg', '.woff', '.js']):
                        endpoints.append(match)
        
    except Exception as e:
        default_logger.debug(f"æ­£åˆ™åˆ†æ JS å¤±è´¥: {e}")
    
    return list(set(endpoints))  # å»é‡
