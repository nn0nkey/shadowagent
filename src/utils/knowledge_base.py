"""
RAGçŸ¥è¯†åº“ç³»ç»Ÿ
æä¾›æ”»å‡»åœºæ™¯çŸ¥è¯†æ£€ç´¢å’Œå¢å¼º
"""
import os
from typing import List, Optional, Dict, Any
from pathlib import Path
import pickle
import json

try:
    import faiss
    from sentence_transformers import SentenceTransformer
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    print("è­¦å‘Š: faiss-cpu æˆ– sentence-transformers æœªå®‰è£…ï¼ŒçŸ¥è¯†åº“åŠŸèƒ½å°†å—é™")

from src.utils.logger import default_logger


class KnowledgeBase:
    """
    RAGçŸ¥è¯†åº“
    
    åŠŸèƒ½ï¼š
    1. åŠ è½½æ”»å‡»åœºæ™¯çŸ¥è¯†æ–‡æ¡£
    2. å‘é‡åŒ–å­˜å‚¨
    3. ç›¸ä¼¼åº¦æ£€ç´¢
    4. æ ¼å¼åŒ–è¾“å‡º
    """
    
    def __init__(
        self,
        knowledge_dir: Optional[Path] = None,
        embedding_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        cache_dir: Optional[Path] = None
    ):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“
        
        Args:
            knowledge_dir: çŸ¥è¯†åº“æ–‡æ¡£ç›®å½•
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            cache_dir: ç¼“å­˜ç›®å½•
        """
        if not FAISS_AVAILABLE:
            default_logger.warning("çŸ¥è¯†åº“åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·å®‰è£… faiss-cpu å’Œ sentence-transformers")
            self.enabled = False
            return
        
        self.enabled = True
        
        # è®¾ç½®è·¯å¾„
        project_root = Path(__file__).parent.parent.parent
        self.knowledge_dir = knowledge_dir or (project_root / "knowledge")
        self.cache_dir = cache_dir or (project_root / "knowledge" / "cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.metadata_map = self._load_metadata()
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        default_logger.info(f"åŠ è½½åµŒå…¥æ¨¡å‹: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        
        # çŸ¥è¯†åº“æ•°æ®
        self.documents: List[Dict[str, Any]] = []
        self.index = None
        
        # åŠ è½½æˆ–æ„å»ºç´¢å¼•
        self._load_or_build_index()
    
    def _load_or_build_index(self):
        """åŠ è½½ç°æœ‰ç´¢å¼•æˆ–æ„å»ºæ–°ç´¢å¼•"""
        index_file = self.cache_dir / "knowledge.faiss"
        metadata_file = self.cache_dir / "knowledge.pkl"
        
        if index_file.exists() and metadata_file.exists():
            try:
                default_logger.info("åŠ è½½å·²æœ‰çŸ¥è¯†åº“ç´¢å¼•...")
                self.index = faiss.read_index(str(index_file))
                with open(metadata_file, 'rb') as f:
                    self.documents = pickle.load(f)
                default_logger.info(f"å·²åŠ è½½ {len(self.documents)} æ¡çŸ¥è¯†")
                return
            except Exception as e:
                default_logger.warning(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}ï¼Œå°†é‡æ–°æ„å»º")
        
        # æ„å»ºæ–°ç´¢å¼•
        self._build_index()
    
    def _build_index(self):
        """æ„å»ºçŸ¥è¯†åº“ç´¢å¼•"""
        default_logger.info("æ„å»ºçŸ¥è¯†åº“ç´¢å¼•...")
        
        # åŠ è½½çŸ¥è¯†æ–‡æ¡£
        self._load_documents()
        
        if not self.documents:
            default_logger.warning("æœªæ‰¾åˆ°çŸ¥è¯†æ–‡æ¡£ï¼ŒçŸ¥è¯†åº“ä¸ºç©º")
            return
        
        # ç”ŸæˆåµŒå…¥å‘é‡
        texts = [doc["content"] for doc in self.documents]
        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)
        
        # åˆ›å»ºFAISSç´¢å¼•
        self.index = faiss.IndexFlatL2(self.embedding_dim)
        self.index.add(embeddings.astype('float32'))
        
        # ä¿å­˜ç´¢å¼•
        index_file = self.cache_dir / "knowledge.faiss"
        metadata_file = self.cache_dir / "knowledge.pkl"
        
        faiss.write_index(self.index, str(index_file))
        with open(metadata_file, 'wb') as f:
            pickle.dump(self.documents, f)
        
        default_logger.info(f"çŸ¥è¯†åº“ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(self.documents)} æ¡çŸ¥è¯†")
    
    def _load_documents(self):
        """åŠ è½½çŸ¥è¯†æ–‡æ¡£"""
        if not self.knowledge_dir.exists():
            default_logger.warning(f"çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {self.knowledge_dir}")
            return
        
        # æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶ï¼ˆåŒ…å«å­ç›®å½•ï¼‰
        md_files = [
            p for p in self.knowledge_dir.rglob("*.md")
            if "cache" not in p.parts  # è·³è¿‡ç¼“å­˜ç›®å½•
        ]
        metadata_map = self.metadata_map or {}
        
        for md_file in md_files:
            if md_file.name.lower() == "readme.md":
                # READMEä»…åšè¯´æ˜ï¼Œä¸çº³å…¥æ£€ç´¢
                continue
            try:
                content = md_file.read_text(encoding='utf-8')
                relative_path = md_file.relative_to(self.knowledge_dir)
                metadata = metadata_map.get(md_file.stem, {})
                
                # è§£ææ–‡æ¡£ï¼ˆç®€å•å®ç°ï¼Œå¯ä»¥æ›´å¤æ‚ï¼‰
                doc = {
                    "id": metadata.get("doc_id", md_file.stem),
                    "title": metadata.get("title") or self._extract_title(content),
                    "content": content,
                    "file_path": str(relative_path),
                    "vuln_type": metadata.get("vuln_type"),
                    "tags": metadata.get("tags", []),
                    "summary": metadata.get("summary"),
                    "difficulty": metadata.get("difficulty"),
                    "metadata": metadata,
                    "source": metadata.get("file_path", str(relative_path))
                }
                
                self.documents.append(doc)
            except Exception as e:
                default_logger.warning(f"åŠ è½½æ–‡æ¡£å¤±è´¥ {md_file}: {e}")
        
        default_logger.info(f"å·²åŠ è½½ {len(self.documents)} ä¸ªçŸ¥è¯†æ–‡æ¡£")
    
    def _load_metadata(self) -> Dict[str, Dict[str, Any]]:
        """åŠ è½½çŸ¥è¯†åº“å…ƒæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
        metadata_map: Dict[str, Dict[str, Any]] = {}
        if not self.knowledge_dir.exists():
            return metadata_map
        
        metadata_files = list(self.knowledge_dir.rglob("knowledge_metadata.json"))
        for meta_file in metadata_files:
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    metadata_map.update(data)
            except Exception as e:
                default_logger.warning(f"åŠ è½½å…ƒæ•°æ®å¤±è´¥ {meta_file}: {e}")
        
        if metadata_map:
            default_logger.info(f"å·²åŠ è½½ {len(metadata_map)} æ¡çŸ¥è¯†å…ƒæ•°æ®")
        return metadata_map
    
    def _extract_title(self, content: str) -> str:
        """ä»Markdownå†…å®¹ä¸­æå–æ ‡é¢˜"""
        lines = content.split('\n')
        for line in lines:
            if line.startswith('# '):
                return line[2:].strip()
        return "æœªå‘½åæ–‡æ¡£"
    
    def search(
        self,
        query: str,
        top_k: int = 5,
        vulnerability_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸å…³çŸ¥è¯†
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰Kä¸ªç»“æœ
            vulnerability_type: æ¼æ´ç±»å‹è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            ç›¸å…³çŸ¥è¯†åˆ—è¡¨
        """
        if not self.enabled or not self.index:
            return []
        
        if not self.documents:
            default_logger.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— æ³•æœç´¢")
            return []
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_model.encode([query])
        
        # æœç´¢
        k = min(top_k, len(self.documents))
        distances, indices = self.index.search(query_embedding.astype('float32'), k)
        
        # æ„å»ºç»“æœ
        results = []
        for idx, distance in zip(indices[0], distances[0]):
            if idx < len(self.documents):
                doc = self.documents[idx].copy()
                doc["similarity_score"] = float(1 / (1 + distance))  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                results.append(doc)
        
        # æŒ‰æ¼æ´ç±»å‹è¿‡æ»¤ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if vulnerability_type:
            results = [
                r for r in results
                if vulnerability_type.lower() in r.get("content", "").lower()
            ]
        
        return results
    
    def format_search_results(
        self,
        query: str,
        results: List[Dict[str, Any]],
        max_length: int = 2000
    ) -> str:
        """
        æ ¼å¼åŒ–æœç´¢ç»“æœ
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            results: æœç´¢ç»“æœ
            max_length: æœ€å¤§è¾“å‡ºé•¿åº¦
        
        Returns:
            æ ¼å¼åŒ–åçš„æ–‡æœ¬
        """
        if not results:
            return ""
        
        formatted_parts = [f"## ğŸ“š ç›¸å…³çŸ¥è¯†æ£€ç´¢ï¼ˆæŸ¥è¯¢: {query}ï¼‰\n"]
        
        for i, result in enumerate(results[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
            title = result.get("title", "æœªå‘½å")
            content = result.get("content", "")
            score = result.get("similarity_score", 0)
            
            # æˆªæ–­å†…å®¹
            if len(content) > 500:
                content = content[:500] + "..."
            
            formatted_parts.append(
                f"### {i}. {title} (ç›¸ä¼¼åº¦: {score:.2f})\n\n{content}\n"
            )
        
        formatted_text = "\n".join(formatted_parts)
        
        # å¦‚æœå¤ªé•¿ï¼Œæˆªæ–­
        if len(formatted_text) > max_length:
            formatted_text = formatted_text[:max_length] + "\n..."
        
        return formatted_text


# å…¨å±€çŸ¥è¯†åº“å®ä¾‹
_knowledge_base: Optional[KnowledgeBase] = None


def get_knowledge_base() -> KnowledgeBase:
    """è·å–å…¨å±€çŸ¥è¯†åº“å®ä¾‹"""
    global _knowledge_base
    if _knowledge_base is None:
        _knowledge_base = KnowledgeBase()
    return _knowledge_base

