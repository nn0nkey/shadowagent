"""
å…¨å±€è§£æå™¨ç®¡ç†å™¨
åœ¨æ¯æ¬¡å·¥å…·æ‰§è¡Œåè‡ªåŠ¨è§£æå“åº”ï¼Œæå–å…³é”®ä¿¡æ¯
æ”¯æŒç¼“å­˜ï¼Œç›¸åŒå“åº”åªè§£æä¸€æ¬¡
"""
import hashlib
from typing import Dict, Optional
from src.utils.rule_based_extractor import RuleBasedExtractor
from src.utils.logger import default_logger
from pathlib import Path


class GlobalParserManager:
    """å…¨å±€è§£æå™¨ç®¡ç†å™¨"""
    
    def __init__(self, rules_file: Optional[str] = None):
        """
        åˆå§‹åŒ–å…¨å±€è§£æå™¨
        
        Args:
            rules_file: è§„åˆ™æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ HaE è§„åˆ™
        """
        if rules_file is None:
            # é»˜è®¤ä½¿ç”¨ HaE çš„è§„åˆ™æ–‡ä»¶
            hae_rules = Path(__file__).parent.parent.parent.parent / 'HaE-main' / 'src' / 'main' / 'resources' / 'rules' / 'Rules.yml'
            if hae_rules.exists():
                rules_file = str(hae_rules)
                default_logger.info(f"ğŸ” ä½¿ç”¨ HaE è§„åˆ™æ–‡ä»¶: {rules_file}")
            else:
                # ä½¿ç”¨è‡ªå®šä¹‰è§„åˆ™
                rules_file = Path(__file__).parent / 'extraction_rules.yaml'
                default_logger.info(f"ğŸ” ä½¿ç”¨è‡ªå®šä¹‰è§„åˆ™æ–‡ä»¶: {rules_file}")
        
        self.extractor = RuleBasedExtractor(rules_file)
        self.cache = {}  # ç¼“å­˜ï¼šresponse_hash -> è§£æç»“æœ
        self.enabled = True
        self.seen_items = {  # å…¨å±€å»é‡ï¼šå·²æå–è¿‡çš„ä¿¡æ¯
            'credentials': set(),
            'privilege_fields': set(),
            'idor_points': set(),
            'fingerprints': set(),
            'vulnerabilities': set(),
        }
    
    def _hash_response(self, response: str) -> str:
        """è®¡ç®—å“åº”çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºç¼“å­˜ï¼‰"""
        return hashlib.md5(response.encode('utf-8')).hexdigest()
    
    def _deduplicate_results(self, results: Dict) -> Dict:
        """
        å…¨å±€å»é‡ï¼šç§»é™¤å·²ç»æå–è¿‡çš„ä¿¡æ¯
        
        Args:
            results: è§£æç»“æœ
            
        Returns:
            å»é‡åçš„ç»“æœ
        """
        deduplicated = {}
        
        for key, items in results.items():
            if not isinstance(items, list):
                deduplicated[key] = items
                continue
            
            unique_items = []
            
            for item in items:
                # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
                if key == 'credentials':
                    if 'username' in item and 'password' in item:
                        item_id = f"{item['username']}:{item['password']}"
                    elif 'type' in item:
                        item_id = f"{item['type']}:{item.get('value', '')[:20]}"
                    else:
                        item_id = str(item)
                
                elif key == 'privilege_fields':
                    item_id = item.get('field', str(item))
                
                elif key == 'idor_points':
                    item_id = item.get('id', str(item))
                
                elif key == 'fingerprints':
                    item_id = f"{item.get('name', '')}:{item.get('value', '')[:20]}"
                
                elif key == 'vulnerabilities':
                    item_id = f"{item.get('name', '')}:{item.get('indicator', '')[:20]}"
                
                else:
                    item_id = str(item)[:50]
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if key in self.seen_items:
                    if item_id not in self.seen_items[key]:
                        self.seen_items[key].add(item_id)
                        unique_items.append(item)
                    else:
                        default_logger.debug(f"ğŸ”„ è·³è¿‡é‡å¤é¡¹: {key} - {item_id[:30]}")
                else:
                    unique_items.append(item)
            
            deduplicated[key] = unique_items
        
        return deduplicated
    
    def parse(self, response: str, force: bool = False) -> Dict:
        """
        è§£æå“åº”å†…å®¹
        
        Args:
            response: å·¥å…·è¾“å‡º/HTTPå“åº”
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°è§£æï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
            
        Returns:
            è§£æç»“æœå­—å…¸
        """
        if not self.enabled:
            return {}
        
        # æ£€æŸ¥ç¼“å­˜
        response_hash = self._hash_response(response)
        if not force and response_hash in self.cache:
            default_logger.debug(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„è§£æç»“æœ (hash: {response_hash[:8]}...)")
            return self.cache[response_hash]
        
        # æ‰§è¡Œè§£æ
        try:
            default_logger.debug(f"ğŸ” å¼€å§‹è§£æå“åº” (é•¿åº¦: {len(response)} å­—ç¬¦)")
            results = self.extractor.extract(response)
            
            # å…¨å±€å»é‡ï¼šç§»é™¤å·²ç»æå–è¿‡çš„ä¿¡æ¯ â­
            results = self._deduplicate_results(results)
            
            # ç»Ÿè®¡æå–åˆ°çš„ä¿¡æ¯
            total_items = sum(len(v) for v in results.values() if isinstance(v, list))
            if total_items > 0:
                default_logger.info(f"âœ… æå–åˆ° {total_items} æ¡æ–°çš„å…³é”®ä¿¡æ¯")
                
                # æ˜¾ç¤ºæ‘˜è¦
                summary_parts = []
                if results.get('credentials'):
                    summary_parts.append(f"å‡­è¯Ã—{len(results['credentials'])}")
                if results.get('fingerprints'):
                    summary_parts.append(f"æŒ‡çº¹Ã—{len(results['fingerprints'])}")
                if results.get('vulnerabilities'):
                    summary_parts.append(f"æ¼æ´Ã—{len(results['vulnerabilities'])}")
                if results.get('secrets'):
                    summary_parts.append(f"æ•æ„Ÿä¿¡æ¯Ã—{len(results['secrets'])}")
                if results.get('api_endpoints'):
                    summary_parts.append(f"APIÃ—{len(results['api_endpoints'])}")
                
                if summary_parts:
                    default_logger.info(f"   ğŸ“Š {', '.join(summary_parts)}")
            
            # ç¼“å­˜ç»“æœ
            self.cache[response_hash] = results
            
            return results
        except Exception as e:
            default_logger.warning(f"âš ï¸ è§£æå¤±è´¥: {e}")
            return {}
    
    def get_summary(self, results: Dict) -> str:
        """è·å–è§£æç»“æœçš„å¯è¯»æ‘˜è¦"""
        return self.extractor.to_summary(results)
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        default_logger.info("ğŸ—‘ï¸ å·²æ¸…ç©ºè§£æç¼“å­˜")
    
    def clear_seen_items(self):
        """æ¸…ç©ºå»é‡è®°å½•ï¼ˆç”¨äºæ–°ä»»åŠ¡ï¼‰"""
        for key in self.seen_items:
            self.seen_items[key].clear()
        default_logger.info("ğŸ—‘ï¸ å·²æ¸…ç©ºå»é‡è®°å½•")
    
    def clear_all(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜å’Œå»é‡è®°å½•"""
        self.clear_cache()
        self.clear_seen_items()
    
    def enable(self):
        """å¯ç”¨å…¨å±€è§£æ"""
        self.enabled = True
        default_logger.info("âœ… å…¨å±€è§£æå·²å¯ç”¨")
    
    def disable(self):
        """ç¦ç”¨å…¨å±€è§£æ"""
        self.enabled = False
        default_logger.info("â¸ï¸ å…¨å±€è§£æå·²ç¦ç”¨")
    
    def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        return {
            'cache_size': len(self.cache),
            'total_items': sum(
                sum(len(v) for v in result.values() if isinstance(v, list))
                for result in self.cache.values()
            )
        }


# å…¨å±€å•ä¾‹
_global_parser = None

def get_global_parser() -> GlobalParserManager:
    """è·å–å…¨å±€è§£æå™¨å®ä¾‹"""
    global _global_parser
    if _global_parser is None:
        _global_parser = GlobalParserManager()
    return _global_parser


def parse_response(response: str, force: bool = False) -> Dict:
    """
    ä¾¿æ·å‡½æ•°ï¼šè§£æå“åº”
    
    Args:
        response: å“åº”å†…å®¹
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°è§£æ
        
    Returns:
        è§£æç»“æœ
    """
    parser = get_global_parser()
    return parser.parse(response, force=force)


def get_parsed_summary(results: Dict) -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šè·å–è§£æç»“æœæ‘˜è¦
    
    Args:
        results: è§£æç»“æœ
        
    Returns:
        å¯è¯»æ‘˜è¦
    """
    parser = get_global_parser()
    return parser.get_summary(results)
